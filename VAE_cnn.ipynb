{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=T.Compose(\n",
    "        [T.Resize(32), T.ToTensor()]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_image = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_size=128):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder\n",
    "        self.e_conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.e_pool = nn.MaxPool2d(2, 2)\n",
    "        self.e_conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.e_fc = nn.Linear(16 * 5 * 5, 300)\n",
    "        self.e_fc_mu = nn.Linear(300, 128)\n",
    "        self.e_fc_log_var = nn.Linear(300, 128)\n",
    "\n",
    "        # decoder\n",
    "        self.d_conv1 = nn.ConvTranspose2d(latent_size, 64, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.d_norm1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.d_conv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.d_norm2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.d_conv3 = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.d_norm3 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.d_conv4 = nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        \n",
    "    def encoder(self, x):\n",
    "        x = self.e_pool(F.relu(self.e_conv1(x)))\n",
    "        x = self.e_pool(F.relu(self.e_conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.e_fc(x))\n",
    "        return self.e_fc_mu(x), self.e_fc_log_var(x)\n",
    "    \n",
    "\n",
    "    def decoder(self, z):\n",
    "        z = F.relu(self.d_norm1(self.d_conv1(z)))\n",
    "        z = F.relu(self.d_norm2(self.d_conv2(z)))\n",
    "        z = F.relu(self.d_norm3(self.d_conv3(z)))\n",
    "        z = F.sigmoid(self.d_conv4(z))\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        \n",
    "        # Reparameterize\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + (eps * std)\n",
    "                \n",
    "        # Add dimensions for conv\n",
    "        z = z.view(z.shape[0], z.shape[1], 1, 1)\n",
    "\n",
    "        return self.decoder(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(net, train_data, batch_size=10, learning_rate=0.0001, epochs=10):\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCELoss(reduction='sum')\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    running_loss = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch:\" , epoch+1)\n",
    "\n",
    "        for bi, data in tqdm(enumerate(train_loader), total=int(len(train_data)/train_loader.batch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "\n",
    "            # Forward\n",
    "            outputs, mu, log_var = net(inputs)\n",
    "            \n",
    "            # Backward\n",
    "            kld = -0.5 * (1 + log_var - mu **2 - log_var.exp()).sum() # Kullback–Leibler divergence\n",
    "            loss = criterion(outputs, inputs) + kld\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss.append(loss.item())\n",
    "        \n",
    "        latent_inputs = torch.randn(64, 128, 1, 1, device=device)\n",
    "        generated_img = net.decoder(latent_inputs)\n",
    "        print(generated_img.shape)\n",
    "        generated_img = make_grid(generated_img)\n",
    "    \n",
    "        #SAVE IMAGE\n",
    "        im = Image.fromarray(np.array(to_pil_image(generated_img)))\n",
    "        im.save(f\"Images_output/vae_cnn/epoch_{epoch}.jpeg\")\n",
    "\n",
    "        print(f'Loss: {np.mean(running_loss[-len(train_data):])}')\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (e_conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (e_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e_conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (e_fc): Linear(in_features=400, out_features=300, bias=True)\n",
       "  (e_fc_mu): Linear(in_features=300, out_features=128, bias=True)\n",
       "  (e_fc_log_var): Linear(in_features=300, out_features=128, bias=True)\n",
       "  (d_conv1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "  (d_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (d_conv2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (d_norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (d_conv3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (d_norm3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (d_conv4): ConvTranspose2d(16, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = VAE()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/6000 [00:00<02:06, 47.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [01:13<00:00, 81.67it/s]\n",
      "  0%|          | 8/6000 [00:00<01:17, 77.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "Loss: 2711.6575600992837\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 4441/6000 [00:55<00:19, 80.59it/s]"
     ]
    }
   ],
   "source": [
    "loss = train(net, train_data, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
